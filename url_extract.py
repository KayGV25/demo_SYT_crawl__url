import yake
import streamlit as st
import article_crawl as ac
from flashtext import KeywordProcessor
from collections import Counter

DEFAULT_KEYWORDS = ["m·ªõi nh·∫•t", "hi·ªán ƒë·∫°i nh·∫•t", "ƒë·ªôc quy·ªÅn", "duy nh·∫•t", "ho√†n to√†n", "nh·∫•t", "ho√†n to√†n 100%"]
name_keywords = ["t√™n", "trung t√¢m", "b·ªánh vi·ªán", "c∆° s·ªü", "ph√≤ng kh√°m"]
ADDRESS_KEYWORDS = ["ƒë·ªãa ch·ªâ", "tr·ª• s·ªü", "chi nh√°nh", "c∆° s·ªü", "khu v·ª±c", "addr", "add", "ƒë.c", "ƒëc", "ƒë/c"]
VIETNAM_CITIES = [
    "H√† N·ªôi", "HN", "H·ªì Ch√≠ Minh", "TPHCM", "HCM", "ƒê√† N·∫µng", "DN", "ƒêN",
    "H·∫£i Ph√≤ng", "HP", "C·∫ßn Th∆°", "CT", "Nha Trang", "NT", "V≈©ng T√†u", "VT"
]
def main(): 
    # st.set_page_config(page_title="Demo L·ªçc T·ª´ kh√≥a Vi ph·∫°m", page_icon="üîç")
    st.title("Demo L·ªçc t·ª´ kh√≥a vi ph·∫°m t·ª´ url")

    url = st.text_input("Nh·∫≠p URL", placeholder="https://www.example.com")    
    filter_btn = st.button(label="L·ªçc", use_container_width=True)
    if filter_btn:
        show_result(url=url, keywords=DEFAULT_KEYWORDS)

def extract_keywords(article: str, keywords: list[str]) -> dict[str, int]:
    keyword_processor = KeywordProcessor()
    keyword_processor.add_keywords_from_list(keywords)
    extracted_keywords = keyword_processor.extract_keywords(article)
    
    # Count the frequency of each keyword
    keyword_frequency = dict(Counter(extracted_keywords))
    
    return keyword_frequency

def show_result(url:str, keywords: list[str]):
    # Ki·ªÉm tra n·∫øu URL v√† t·ª´ kh√≥a kh√¥ng ƒë∆∞·ª£c nh·∫≠p
    if not url:
        st.error("Vui l√≤ng nh·∫≠p ƒë·∫ßy ƒë·ªß th√¥ng tin.")
    else:
        article = ac.crawl_and_clean_article(url=url)

        filtered_keywords = extract_keywords(article["content"], keywords)
        sentences_with_keywords = get_sentences_with_keywords(article, filtered_keywords.keys())
        is_violated = article["address"] is None or article["place_name"] is None

        if len(filtered_keywords) == 0:
            st.success("Kh√¥ng ph√°t hi·ªán vi ph·∫°m v·ªÅ t·ª´ ng·ªØ c·∫•m (ƒêi·ªÅu 8, √ù 11).")
        else:
            # st.subheader("Danh s√°ch c√¢u c√≥ ch·ª©a t·ª´ kh√≥a vi ph·∫°m")
            st.subheader("**Vi ph·∫°m ƒêi·ªÅu 8, √ù 11:** S·ª≠ d·ª•ng t·ª´ ng·ªØ c·∫•m ho·∫∑c c√≥ √Ω nghƒ©a t∆∞∆°ng t·ª±\n")
            st.text_area("Danh s√°ch c√¢u c√≥ ch·ª©a t·ª´ kh√≥a vi ph·∫°m","- " + "\n- ".join([f"{s}" for s in sentences_with_keywords]), height=300)
            st.error("K·∫øt qu·∫£ l·ªçc <t·ª´ kh√≥a>: <s·ªë l·∫ßn xu·∫•t hi·ªán>:\n- " + "\n- ".join([f"{kw}: {f}" for kw, f in filtered_keywords.items()]))
        if is_violated:
            st.write("**Vi ph·∫°m ƒêi·ªÅu 9, √ù 2a:** Thi·∫øu th√¥ng tin b·∫Øt bu·ªôc c·ªßa c∆° s·ªü kh√°m b·ªánh, ch·ªØa b·ªánh")
            if article["address"] is None:
                st.error(" - ƒê·ªãa ch·ªâ kh√¥ng ƒë∆∞·ª£c cung c·∫•p.")
            if article["place_name"] is None:
                st.error(" - T√™n ƒë·ªãa ƒëi·ªÉm kh√¥ng ƒë∆∞·ª£c cung c·∫•p.")
        else:
            st.success("ƒê√£ c√≥ ƒë·∫ßy ƒë·ªß th√¥ng tin b·∫Øt bu·ªôc (ƒêi·ªÅu 9, √ù 2a).")
        
        st.subheader("K·∫øt qu·∫£ ph√¢n lo·∫°i:")
        if is_violated or len(filtered_keywords) > 0:
            st.error("Sai lu·∫≠t")
        else:
            st.success("ƒê√∫ng lu·∫≠t")

        # st.success(f"T·ª´ kh√≥a kh√°c trong b√†i vi·∫øt - Yake: \n- " + "\n- ".join(topYake(article, DEFAULT_KEYWORDS.lower().split(", "), filtered=filtered_options, length=length_keywords_list)))

def get_sentences_with_keywords(article: ac._article, keywords: list[str]) -> list[str]:
    sentences = article["content"].split(". ")
    sentences_with_keywords = []
    for sentence in sentences:
        if any(keyword.lower() in sentence.lower() for keyword in keywords):
            sentences_with_keywords.append(sentence)
    return list(set(sentences_with_keywords))


# def topYake(article, word_list:list[str], length:int=10, filtered:bool=False):
#     kw_extractor = yake.KeywordExtractor(lan="vi", top=100)
#     article_keywords = kw_extractor.extract_keywords(article["content"] + ". " + article["title"])
#     sorted_article_keywords = sorted(article_keywords, key=lambda x: x[1], reverse=False)

#     if not filtered:
#         top = [kw for kw, score in sorted_article_keywords]
#         return top[:length]
#     filtered_keywords = [kw for kw, score in sorted_article_keywords if any(word.lower() in kw.lower() for word in word_list)]
#     return filtered_keywords[:length]

# def top10KeyBert(article):
#     kw_extractor = KeyBERT()
#     article_keywords = kw_extractor.extract_keywords((article["title"] + " " + article["content"]).lower(), keyphrase_ngram_range=(1, 3), stop_words='vietnamese', top_n=10)
#     top10 = [kw for kw, score in article_keywords]
#     return top10

main()
